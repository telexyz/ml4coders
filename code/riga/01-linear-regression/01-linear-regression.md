### Linear Regression

_ _ _

##### MÃ” Táº¢

**Linear Regression** - **Há»“i quy tuyáº¿n tÃ­nh** lÃ  thuáº­t Ä‘oÃ¡n Ä‘Æ¡n giáº£n nháº¥t Ä‘á»ƒ viáº¿t má»™t mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n tráº£ vá» káº¿t quáº£ sá»‘ thá»±c nhÆ° giÃ¡ nhÃ  cá»­a, chiá»u cao, cÃ¢n náº·ng, hay giÃ¡ cá»• phiáº¿u giÃ¡ bitcoin (just for fun), etc.

* Æ¯u Ä‘iá»ƒm
  
  * ÄÆ¡n giáº£n
  
  * Dá»… hiá»ƒu
  
  * CÃ i Ä‘áº·t nhanh

* NhÆ°á»£c Ä‘iá»ƒm
  
  * Ráº¥t nháº£y cáº£m vá»›i dá»¯ liá»‡u nhiá»…u
  
  * KhÃ´ng linh hoáº¡t
  
  * NÃªn káº¿t quáº£ khÃ´ng Ä‘áº¹p
  
  * Do Ä‘Ã³ tÃ­nh á»©ng dá»¥ng thá»±c táº¿ khÃ´ng nhiá»u

**BÃ i toÃ¡n**

Tiáº¿p cáº­n Ä‘Æ¡n giáº£n vÃ  hiá»ƒu theo cÃ¡ch cá»§a há»c sinh cáº¥p 3 thÃ¬ bÃ i toÃ¡n sáº½ nhÆ° sau:

CÃ³ má»™t Ä‘á»‘ng dá»¯ liá»‡u, á»Ÿ Ä‘Ã¢y lÃ  cÃ¡c Ä‘iá»ƒm trÃªn má»™t máº·t pháº³ng. TÃ¬m cÃ¡ch váº½ má»™t Ä‘Æ°á»ng tháº³ng Ä‘i qua Ä‘á»‘ng dá»¯ liá»‡u Ä‘Ã³ sao cho Ä‘Æ°á»ng tháº³ng tiáº¿p cáº­n gáº§n nháº¥t vá»›i cÃ¡c Ä‘iá»ƒm Ä‘Ã³.

**CÃ¡ch giáº£i**

PhÆ°Æ¡ng trÃ¬nh Ä‘Æ°á»ng tháº³ng sáº½ cÃ³ dáº¡ng `F(x) = Î²x + Î±`. Ta cáº§n tÃ¬m lÃ  hai há»‡ sá»‘ `Î² & Î±`. 

Äá»ƒ Ä‘Æ°á»ng tháº³ng tiáº¿p cáº­n gáº§n cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u nháº¥t thÃ¬ sai sá»‘ pháº£i nhá» nháº¥t, sai sá»‘ á»Ÿ Ä‘Ã¢y chÃ­nh lÃ  tá»•ng cÃ¡c delta khoáº£ng cÃ¡ch cÃ¡c Ä‘iá»ƒm Ä‘áº¿n Ä‘Æ°á»ng tháº³ng, tÃ­nh theo `F(x)` cá»§a ta, vÃ  khoáº£ng cÃ¡ch thá»±c táº¿ cÃ¡c Ä‘iá»ƒm Ä‘Ã³ (ta gá»i nÃ³ lÃ  `y`), lÃ  nhá» nháº¥t. Tá»©c lÃ  ta cÃ³ `âˆ‘(Fğ—‘áµ¢ - Yáµ¢)` nhá» nháº¥t.

![](screenshots/lr-1.png)

á» cáº¥p 3 chÃºng ta Ä‘Ã£ Ä‘Æ°á»£c há»c tÃ¬m nghiá»‡m cho má»™t bÃ i toÃ¡n tá»‘i Æ°u ta cÃ³ thá»ƒ dÃ¹ng Ä‘áº¡o hÃ m, nhÆ°ng phÆ°Æ¡ng trÃ¬nh nÃ y báº­c nháº¥t, Ä‘áº¡o hÃ m thÃ¬ biáº¿n bá»‹ triá»‡t tiÃªu rá»“i thÃ¬ lÃ m sao? á» Ä‘Ã¢y, ta cÃ³ cÃ¡ch lÃ  bÃ¬nh phÆ°Æ¡ng sai sá»‘ lÃªn, tÃ­nh cháº¥t sai sá»‘ nÃ³ váº«n váº­y, vÃ  ta sáº½ tÃ¬m nghiá»‡m ngon lÃ nh, cÃ¡c cá»¥ nhÃ  ta gá»i cÃ¡ch nÃ y lÃ  `Squared Error aka SE`. NhÆ° váº­y, ta sáº½ cÃ³ `J = âˆ‘(Fğ—‘áµ¢ - Yáµ¢)Â²` = `âˆ‘((Î²*xáµ¢ + Î±) - yáµ¢)Â²`. ÄÆ¡n giáº£n hÃ³a ta táº¡m coi `Î± = 0` (ko pháº£i bá» Ä‘i, sáº½ cÃ³ á»Ÿ dÆ°á»›i).

Sau khi Ä‘áº¡o hÃ m ta cÃ³ `J' = 2(âˆ‘Î²xáµ¢Â² - âˆ‘xáµ¢yáµ¢)` vÃ  Ä‘Ã¢y chÃ­nh lÃ  hÃ m loss cá»§a chÃºng ta.

Giáº£i phÆ°Æ¡ng trÃ¬nh nÃ y tÃ¬m nghiá»‡m `J' = 0 <=> âˆ‘Î²xáµ¢Â² - âˆ‘xáµ¢yáµ¢ = 0`.

Ta sáº½ tÃ¬m ra `Î² = âˆ‘xáµ¢yáµ¢ Ã· âˆ‘xáµ¢Â²`vÃ  `Î± = Fxáµ¢ - Î²xáµ¢`

#### CHÆ¯Æ NG TRÃŒNH

> $ pip install --ignore-installed -r requirements.txt
> 
> $ python 01-linear-regression.py

##### Káº¾T QUáº¢

![](screenshots/test-1.png)
![](screenshots/test-2.png)

*P.S KhÃ´ng pháº£i dÃ¢n ML. LÃ  tay mÆ¡ vá» IT, thá»i gian ráº£nh tÃ¬m hiá»ƒu ML cÃ¹ng anh Tuá»™c, cÃ³ gÃ¬ khÃ´ng Ä‘Ãºng ACE cá»© tháº³ng tháº¯ng bÃ¬nh luáº­n hÆ°á»›ng dáº«n chá»‰ báº£o em.*
