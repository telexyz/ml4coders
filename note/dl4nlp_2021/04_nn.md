CÃ¡ch biá»ƒu diá»…n vÄƒn báº£n BOW, TF-IDF ... cÃ³ nhiá»u háº¡n cháº¿ nhÆ° bá» qua ngá»¯ cáº£nh (BOW), sá»‘ chiá»u khÃ´ng gian quÃ¡ to (= vocab size), khÃ´ng cÃ³ sá»± liÃªn há»‡ vá» ngá»¯ nghÄ©a (TF-IDF)

MÃ´ hÃ¬nh ngÃ´n ngá»¯ n-gram (count-based) tuy Ä‘Æ¡n giáº£n vÃ  há»¯u dá»¥ng nhÆ°ng cÃ³ nhiá»u háº¡n cháº¿: OOV (LM nÃ o cÅ©ng pháº£i), ngá»¯ cáº£nh cÃ ng lá»›n (n lá»›n) thÃ¬ cÃ ng khÃ³ handle, ngram count khÃ´ng thá»ƒ hiá»‡n Ä‘Æ°á»£c sá»± liÃªn há»‡ vá» ngá»¯ nghÄ©a ...

- - -

## Featured Model

Trang 10 cá»§a [slides](04_embed.pdf)

IDEA: Thay vÃ¬ coi tá»« chá»‰ lÃ  má»™t sá»‘ bÃ¡o danh trong tá»« Ä‘iá»n, hÃ£y biá»ƒu diá»…n tá»« nhÆ° lÃ  nhiá»u Ä‘áº·c trÆ°ng mÃ : `# of features << # of words in vocab`.

Má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ dá»±a trÃªn features:

![](_files/04_featured-model.png){width=600 height=331}

![](_files/04_featured_model2.png){width=600 height=272}

![](_files/04_featured_model3.png){width=600 height=280}

### How we do train a model to learn these 2 matrices, and the bias vector?

- Use our output probabilities
- Calculate the cross-entropy loss
- Use backprop to calculate gradients
- Update the 2 embedding matrices and bias via Gradient Decent


### Unknown Words

Common ways to amend the data:
â€¢ Frequency threshold (e.g., `UNK <= 2`)
â€¢ Remove bottom N%
â€¢ Represent each word as `sub-words` (e.g., byte-pair encodings)

Common neural modelling approaches:
â€¢ Add an `UNK` token to your vocabulary (just like for `n-grams`)


### Evaluation LMs
 
__Very Important:__

â€¢ Any given LM must be able to generate the test set (at least).
Otherwise, it cannot be fairly evaluated (OOV problem).

â€¢ When comparing multiple LMs to each other, their vocabularies must be the same (e.g., words, sub-words, characters).


### Neural Network Motivation

* __Non-linear power__: using non-linear activation functions can allow us to capture rich, combinatorial attributes of language

* Word embeddings reduce the` # of parameters` and hopefully improve the model's ability to generalize

- - -

### A Neural Probabilistic Language Model. Bengio et al. JMLR (2003)
![](_files/04_Bengio.png)

1/ Thá»ƒ hiá»‡n má»™t tá»« trong bá»™ tá»« vá»±ng tÆ°Æ¡ng á»©ng vá»›i má»™t `feature vector` trong `khÃ´ng gian m chiá»u`

2/ Thá»ƒ hiá»‡n `joint prob function` cá»§a chuá»—i tá»« thÃ nh cá»§a chuá»—i vectors

3/ CÃ¹ng má»™t lÃºc há»c (thay Ä‘á»•i trá»ng sá»‘) `word feature vectors` vÃ  cÃ¡c tham sá»‘ cá»§a hÃ m xÃ¡c xuáº¥t Ä‘Ã³


#### Simultaneously learn the representation and do the modelling!

Words that are more semantically similar to one another will have embeddings that are proportionally similar, too.

![](_files/04_Bengio2.png){width=800 height=445}

- Word embeddings: similar input words get similar vectors
- Similar contexts get similar hidden states
- Similar output words get similar rows in the output matrix ğ‘ˆ

Train the model using gradient descent: 
â€¢ Use our output probabilities
â€¢ Calculate the cross-entropy loss
â€¢ Use backprop to calculate gradients
â€¢ Update all weight matrices and bias via GD

!! SAME AS WE DO FOR ALL OF OUR NEURAL NETS !!

#### Bengio (2003)

This was not the first neural language model, but it was the first, highly compelling model with great results (e.g., beating n-grams)

The softmax output layer is annoyingly slow

- - -

## Distributional Semantics

_Distributional: meaning is represented by the contexts in which its used_

> â€œDistributional statements can cover all of the material of a language without requiring support from other types of informationâ€ -- Zellig Harris (1954)

> â€œYou shall know a word by the company it keepsâ€ -- John Rupert Firth (1957)

### Auto-regressive language models
```
I bought a _____
Good morning, _____ 
I got my ______
```

### Masked language models
```
I bought a _____ from the bakery
Good morning, _____. Rise and shine! 
I got my ______ license last week
```

### word2vec: skip-gram

Step 1: Iterate through your entire corpus, with sliding context windows of size ğ‘µ and step size ğŸ
Step 2: Using the masked center word, try to predict all 2N center words.
Step 3: Calculate your loss and update parameters (like always)

In practice, this softmax is painfully slow. Instead, flip the modelling to be pairs of words:
e.g., (center word, context word)

It would learn to always predict 1. So, probabilistically sample negative examples based on their frequencies. (Negative Sampling)


### word2vec: results

Smaller window sizes yield embeddings such that high similarity scores indicates that the words are interchangeable

Larger window sizes (e.g., 15+) yield embeddings such that high similarity is more indicative of relatedness of the words.

Words that appear in the same contexts are forced to gravitate toward having the same embeddings as one another:

â€¢ Imagine two words, w1 and w2, that never appear together, but they each, individually have the exact same contexts with other words. w1 and w2 will have identical embeddings!

â€¢ â€œTheâ€ appears the most. What do you imagine its embedding is like?


### word2vec: Remaining Challenges

â€¢ Still canâ€™t handle long-range dependencies.
â€¢ Each decision is independent of the previous!
â€¢ Having a small, fixed window that repeats is a bit forced and awkward